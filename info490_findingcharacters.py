# -*- coding: utf-8 -*-
"""INFO490-FindingCharacters

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x5g9aOgWuHN-xdJs91GPorXCWiYTfBGK
"""

import urllib.request
import os
import requests

def build_google_drive_url(doc_id):

  DRIVE1  = "https://docs.google.com/uc" 
  DRIVE2  = "https://drive.google.com/uc"  

  baseurl = DRIVE1 # DRIVE2 works as well 
  params = {"export" : "download",
            "id"     : doc_id}
 
  url = baseurl + "?" + urllib.parse.urlencode(params) 
  return url

def read_google_doc(doc_id): 
   # read the remote document pointed to by doc_id
   # save the contents to a local file
   # return the contents of the document/local file
  url = build_google_drive_url(doc_id)
  r = requests.get(url)
  new_name = doc_id + ".txt"
  
  with open(new_name, 'w') as file:
    file.write(r.text)
  
  f = open(new_name, 'r')
  file_contents = f.read()
  return(file_contents)

import re
import collections

def read_text(filename):
  f = open(filename, 'r')
  file_contents = f.read()
  return file_contents
  
def split_text_into_tokens(text):
  def r_expression():
    pattern =  r"['A-Za-z0-9]+-?['A-Za-z0-9]+"
    return re.compile(pattern, re.IGNORECASE)
  
  result = r_expression().findall(text)
  
  edited_list = []
  
  for word in result:
    if word[0] == "'" and word[-1] == "'":
      word2 = word[1:-1]
      edited_list.append(word2.strip())
    elif word[0] == "'":
      if word[-1] == "'":
        word2 = word[1:-1]
        edited_list.append(word2.strip())
      else:
        word2 = word[1:]
        edited_list.append(word2.strip())
    
    elif word[-1] == "'":
      word2 = word[:-1]
      edited_list.append(word2.strip())
    else:
      edited_list.append(word)
  
  return edited_list

def load_stop_words():
  return ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', "aren't", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', "can't", 'cannot', 'could', "couldn't", 'did', "didn't", 'do', 'does', "doesn't", 'doing', "don't", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', "hadn't", 'has', "hasn't", 'have', "haven't", 'having', 'he', "he'd", "he'll", "he's", 'her', 'here', "here's", 'hers', 'herself', 'him', 'himself', 'his', 'how', "how's", 'i', "i'd", "i'll", "i'm", "i've", 'if', 'in', 'into', 'is', "isn't", 'it', "it's", 'its', 'itself', "let's", 'me', 'more', 'most', "mustn't", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', "shan't", 'she', "she'd", "she'll", "she's", 'should', "shouldn't", 'so', 'some', 'such', 'than', 'that', "that's", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', "there's", 'these', 'they', "they'd", "they'll", "they're", "they've", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', "wasn't", 'we', "we'd", "we'll", "we're", "we've", 'were', "weren't", 'what', "what's", 'when', "when's", 'where', "where's", 'which', 'while', 'who', "who's", 'whom', 'why', "why's", 'with', "won't", 'would', "wouldn't", 'you', "you'd", "you'll", "you're", "you've", 'your', 'yours', 'yourself', 'yourselves']
 
def bi_grams(tokens):
  new_tokens = []
  counter = 0
  
  for counter in range(0, len(tokens)-1):
    word1 = tokens[counter]
    word2 = tokens[counter+1]
    new_tokens.append((word1, word2))
    counter += 1
  return new_tokens

def top_n(tokens, n):
  counter = collections.Counter(tokens)
  return counter.most_common(n)
  
def remove_stop_words(tokens, stoplist):
  final_list = []
  counter = 0
  
  for counter in range(0, len(tokens)-1):
    if tokens[counter].lower() not in stoplist:
      final_list.append(tokens[counter])
      counter+=1
    else:
      counter+=1
  
  return final_list

def find_characters_v1(text, stoplist, top):
  tokens = split_text_into_tokens(text)
  tokens_no_stop = remove_stop_words(tokens, stoplist)

  capital_words = []
  for word in tokens_no_stop:
    if word[0] in "ABCDEFGHIJKLMNOPQRSTUVWXYZ":
      capital_words.append(word)
  
  capital_words_and_counter = top_n(capital_words, top)

  return capital_words_and_counter

# HUCK_ID = "13F68-nA4W-0t3eNuIodh8fxTMZV5Nlpp"
# text = read_google_doc(HUCK_ID)
# stop = load_stop_words()
# v1  = find_characters_v1(text, stop, 15)
# print(v1)

def find_characters_v2(text, stoplist, top):
  tokens = split_text_into_tokens(text)
  bigram_list = bi_grams(tokens)
  capital_bigrams = []
  
  for word in bigram_list:
    if word[0][0].isupper() == True and word[1][0].isupper() == True:
      if word[0].lower() not in stoplist and word[1].lower() not in stoplist:
        capital_bigrams.append(word)
  
  capital_bigrams_and_counter = top_n(capital_bigrams, top)

  return capital_bigrams_and_counter

#  v2  = find_characters_v2(text, [], 15)
#  print(v2)

def get_titles(text):

  def r_titles_period():
    pattern =  r"\b[A-Z]{1}[a-z]{1,3}\.{1}"
    return re.compile(pattern)

  def r_titles_no_period():
    pattern =  r"(\b[A-Z]{1}[a-z]{1,3})\s"
    return re.compile(pattern)
  
  pseudo_titles = r_titles_no_period().findall(text)
  title_tokens = r_titles_period().findall(text)
  title_tokens_minus_period = []
  
  for word in title_tokens:
    new_word = word[0:-1]
    title_tokens_minus_period.append(word[0:-1])

  pseudo_titles_set = set(pseudo_titles)
  title_tokens_minus_period_set = set(title_tokens_minus_period)

  titles = list(title_tokens_minus_period_set.difference(pseudo_titles_set))
  return titles

# HUCK_ID = "13F68-nA4W-0t3eNuIodh8fxTMZV5Nlpp"
# text = read_google_doc(HUCK_ID)
# stop = load_stop_words()
# titles = get_titles(text)
# # print(titles)

def find_characters_v3(text, stoplist, top):
  tokens = split_text_into_tokens(text)
  tokens_no_stop = remove_stop_words(tokens, load_stop_words())
  bigram_list = bi_grams(tokens_no_stop)
  bigram_titles = []
  titles = get_titles(text)

  for word in bigram_list:
    if word[0] in titles:
      if word[0].lower() not in stoplist and word[1].lower() not in stoplist:
        bigram_titles.append(word)
  
  top_titles = top_n(bigram_titles, top)

  return top_titles

# v3  = find_characters_v3(text, load_stop_words(), 15)
# print(v3)

def jupyter():
  url = 'https://drive.google.com/open?id=1x5g9aOgWuHN-xdJs91GPorXCWiYTfBGK'
  return '1x5g9aOgWuHN-xdJs91GPorXCWiYTfBGK'